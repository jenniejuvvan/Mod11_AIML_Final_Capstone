
# Module 17 Practical Application 3: Bank Marketing Dataset

This project focuses on applying machine learning techniques to the **Bank Marketing Dataset** to predict whether a client will subscribe to a term deposit (target variable `y`).

---

## üìå Business Objective
The main business objective is to help the bank improve marketing efficiency by predicting which customers are likely to subscribe to a term deposit.  
This reduces wasted calls, improves customer targeting, and increases campaign effectiveness.

---

## üìä Dataset Overview
The dataset contains both **categorical** and **numeric** features, including client information, contact history, and macroeconomic indicators.  
Target:  
- `y` ‚Üí whether the client subscribed (`yes`/`no`).

---

## üîç Steps Performed

### 1. Data Checks
- Checked for missing values and duplicates.
- Inspected incorrect or impossible values (`pdays`, `previous`, `default`).
- Identified outliers in numeric columns (`age`, `campaign`, `duration`).
- Converted columns to correct datatypes.

### 2. Exploratory Data Analysis (EDA)
- **Univariate**: Histograms, density plots, summary stats.  
- **Bivariate**: Correlation heatmaps, scatterplots, boxplots.  
- **Multivariate**: Combined feature visualizations.  
- Observed imbalance in target (`y`: ~89% no, 11% yes).

### 3. Preprocessing
- **Encoding**: One-Hot Encoding for categorical, Label Encoding for target.  
- **Scaling**: StandardScaler for Logistic Regression/Decision Tree; MinMaxScaler for KNN/SVM.  
- **Imbalance Handling**: SMOTE considered.  
- **Feature Selection**: Used correlation and `SelectKBest` to retain most informative features.  
- **Outlier Handling**: Capped extreme values (e.g., age, campaign).

### 4. Modeling
Built baseline and advanced models:  
- Logistic Regression  
- KNN (K-Nearest Neighbors)  
- Decision Tree  
- SVM (Support Vector Machine)  
- Dummy Classifier (baseline)

### 5. Model Comparison
Each model was compared on:  
- Train time  
- Train accuracy  
- Test accuracy  
- Precision, Recall, F1-score  
- ROC-AUC

### 6. Hyperparameter Tuning
- Used **GridSearchCV** to tune hyperparameters:
  - Logistic Regression (regularization)  
  - KNN (n_neighbors, weights)  
  - Decision Tree (max_depth, min_samples_split)  
  - SVM (kernel, C, gamma)

---

## üìà Results (Sample)
| Model               | Train Time (s) | Train Acc | Test Acc |
|----------------------|----------------|-----------|----------|
| Logistic Regression | 0.12           | 0.89      | 0.87     |
| KNN                 | 0.32           | 0.92      | 0.88     |
| Decision Tree       | 0.05           | 1.00      | 0.85     |
| SVM                 | 1.44           | 0.93      | 0.90     |
| Dummy (baseline)    | 0.01           | 0.88      | 0.88     |

- **Decision Tree** risked overfitting (train=100% accuracy).  
- **SVM** gave the best balance of accuracy and generalization.  
- **Logistic Regression** remained strong and interpretable.  

---

## üöÄ Next Steps
- More feature engineering (binning, interaction terms).  
- Try ensemble methods (Random Forest, Gradient Boosting, XGBoost).  
- Focus on **recall** (reduce false negatives ‚Üí fewer missed opportunities).  
- Deploy the model as an API or dashboard.

---

## üõ†Ô∏è How to Run
1. Open the Jupyter Notebook (`Module_17_Practical_Application_3.ipynb`).  
2. Run the preprocessing cells to clean and encode the dataset.  
3. Execute modeling cells to train and evaluate classifiers.  
4. Review outputs and plots for model performance.  

---

## üìÇ Project Structure
```
‚îú‚îÄ‚îÄ Module_17_Practical_Application_3.ipynb   # Main notebook
‚îú‚îÄ‚îÄ bank+marketing.csv                        # Dataset
‚îú‚îÄ‚îÄ README.md                                 # Project documentation
‚îî‚îÄ‚îÄ images/                                   # Plots & visualizations
```

---

## ‚úÖ Key Learnings
- Data preprocessing (encoding, scaling, outlier handling) is crucial.  
- Always compare multiple models; accuracy is not enough‚Äîcheck precision, recall, F1.  
- Hyperparameter tuning significantly improves model performance.  
- Interpretability matters: Logistic Regression is simple but powerful, while SVM and Trees capture complexity.

